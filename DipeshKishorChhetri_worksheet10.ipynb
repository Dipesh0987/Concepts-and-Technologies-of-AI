{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1qkKu_Ac__-Mc3GGCb6LSn7eR4E-MM_SG",
      "authorship_tag": "ABX9TyPs2FU7U6tRhDswI4k5EWNp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dipesh0987/Concepts-and-Technologies-of-AI/blob/main/DipeshKishorChhetri_worksheet10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqtL0xs7t6IS",
        "outputId": "9393ccc4-7c1f-41a7-d545-90bc397b9c6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset info:\n",
            "Shape: (50000, 2)\n",
            "\n",
            "Column names: ['review', 'sentiment']\n",
            "\n",
            "First review example:\n",
            "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ...\n",
            "\n",
            "Sentiment: positive\n",
            "\n",
            "Sentiment distribution:\n",
            "sentiment\n",
            "positive    25000\n",
            "negative    25000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/IMDB Dataset.csv\")\n",
        "length = len(df['review'])\n",
        "print(\"Dataset info:\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"\\nColumn names: {df.columns.tolist()}\")\n",
        "print(f\"\\nFirst review example:\\n{df['review'].iloc[0][:500]}...\")\n",
        "print(f\"\\nSentiment: {df['sentiment'].iloc[0]}\")\n",
        "print(f\"\\nSentiment distribution:\")\n",
        "print(df['sentiment'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 1:**\n",
        "1. Load the dataset and preprocess the reviews.\n",
        "\n",
        "    a. Convert all text to lowercase.\n",
        "\n",
        "    b. Remove non-alphabetic characters (punctuation).\n",
        "\n",
        "    c. Tokenize the reviews and remove common stopwords.\n",
        "\n",
        "    d. Apply stemming to reduce words to their root form.\n",
        "\n",
        "2. Split the dataset into training and testing sets (80% training, 20% testing).\n",
        "\n",
        "3. Use a Naive Bayes classifier to classify the reviews into positive and negative categories.\n",
        "\n",
        "    a. Implement a Bag-of-Words model using CountVectorizer.\n",
        "\n",
        "    b. Train the Naive Bayes classifier using the training set."
      ],
      "metadata": {
        "id": "DQ1NeyTqzb-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# all reviews to lowercase\n",
        "df['review'] = df['review'].str.lower()\n",
        "print(\"After converting to lowercase:\")\n",
        "print(df['review'].iloc[0][:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zjkv-9rMzEh6",
        "outputId": "633d72fb-9dd9-4281-f79e-d277df3fab5b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After converting to lowercase:\n",
            "one of the other reviewers has mentioned that after watching just 1 oz episode you'll be hooked. they are right, as this is exactly what happened with me.<br /><br />the first thing that struck me about oz was its brutality and unflinching scenes of violence, which set in right from the word go. trust me, this is not a show for the faint hearted or timid. this show pulls no punches with regards to drugs, sex or violence. its is hardcore, in the classic use of the word.<br /><br />it is called oz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Remove non-alphabetic characters and extra whitespace\n",
        "df['review_clean'] = df['review'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))\n",
        "df['review_clean'] = df['review_clean'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
        "\n",
        "print(\"After removing non-alphabetic characters:\")\n",
        "print(df['review_clean'].iloc[0][:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9DUonhdz38W",
        "outputId": "8c8a398a-c8dd-43c8-839e-8bbcd2487518"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After removing non-alphabetic characters:\n",
            "one of the other reviewers has mentioned that after watching just oz episode youll be hooked they are right as this is exactly what happened with mebr br the first thing that struck me about oz was its brutality and unflinching scenes of violence which set in right from the word go trust me this is not a show for the faint hearted or timid this show pulls no punches with regards to drugs sex or violence its is hardcore in the classic use of the wordbr br it is called oz as that is the nickname g\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Get English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Tokenize and remove stopwords\n",
        "df['review_tokens'] = df['review_clean'].apply(lambda x: [word for word in x.split() if word not in stop_words])\n",
        "\n",
        "print(\"After tokenization and removing stopwords:\")\n",
        "print(f\"First review tokens (first 20): {df['review_tokens'].iloc[0][:20]}\")\n",
        "print(f\"Number of tokens in first review: {len(df['review_tokens'].iloc[0])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zssZlbWC0JX4",
        "outputId": "19ca1dd5-e6db-4d21-a0f5-7aa0e0a68120"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After tokenization and removing stopwords:\n",
            "First review tokens (first 20): ['one', 'reviewers', 'mentioned', 'watching', 'oz', 'episode', 'youll', 'hooked', 'right', 'exactly', 'happened', 'mebr', 'br', 'first', 'thing', 'struck', 'oz', 'brutality', 'unflinching', 'scenes']\n",
            "Number of tokens in first review: 170\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('punkt')\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Apply stemming to tokens\n",
        "df['review_stemmed'] = df['review_tokens'].apply(lambda tokens: [stemmer.stem(word) for word in tokens])\n",
        "\n",
        "print(\"After stemming:\")\n",
        "print(f\"First review stemmed tokens (first 20): {df['review_stemmed'].iloc[0][:20]}\")\n",
        "\n",
        "# Join tokens back into strings for the model\n",
        "df['review_processed'] = df['review_stemmed'].apply(lambda tokens: ' '.join(tokens))\n",
        "\n",
        "print(f\"\\nProcessed review (first 500 chars):\")\n",
        "print(df['review_processed'].iloc[0][:500])"
      ],
      "metadata": {
        "id": "pzD8MPuN0onj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbd69ec1-c920-447f-a93f-56173b5f3c44"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After stemming:\n",
            "First review stemmed tokens (first 20): ['one', 'review', 'mention', 'watch', 'oz', 'episod', 'youll', 'hook', 'right', 'exactli', 'happen', 'mebr', 'br', 'first', 'thing', 'struck', 'oz', 'brutal', 'unflinch', 'scene']\n",
            "\n",
            "Processed review (first 500 chars):\n",
            "one review mention watch oz episod youll hook right exactli happen mebr br first thing struck oz brutal unflinch scene violenc set right word go trust show faint heart timid show pull punch regard drug sex violenc hardcor classic use wordbr br call oz nicknam given oswald maximum secur state penitentari focus mainli emerald citi experiment section prison cell glass front face inward privaci high agenda em citi home manyaryan muslim gangsta latino christian italian irish moreso scuffl death stare\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Prepare the features (X) and labels (y)\n",
        "X = df['review_processed']  # Processed text\n",
        "y = df['sentiment']  # Labels (positive/negative)\n",
        "\n",
        "# Convert labels to binary (1 for positive, 0 for negative)\n",
        "y_binary = y.map({'positive': 1, 'negative': 0})\n",
        "\n",
        "# Split the data (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_binary, test_size=0.2, random_state=42, stratify=y_binary\n",
        ")\n",
        "\n",
        "print(\"=== DATA SPLITTING RESULTS ===\")\n",
        "print(f\"Total samples: {len(X)}\")\n",
        "print(f\"Training samples: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
        "print(f\"Testing samples: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n",
        "print(f\"\\nTraining set class distribution:\")\n",
        "print(y_train.value_counts())\n",
        "print(f\"\\nTesting set class distribution:\")\n",
        "print(y_test.value_counts())"
      ],
      "metadata": {
        "id": "di8G_pKB0wL1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d225302-dd9e-41de-a745-bdfed2e826ea"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== DATA SPLITTING RESULTS ===\n",
            "Total samples: 50000\n",
            "Training samples: 40000 (80.0%)\n",
            "Testing samples: 10000 (20.0%)\n",
            "\n",
            "Training set class distribution:\n",
            "sentiment\n",
            "1    20000\n",
            "0    20000\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Testing set class distribution:\n",
            "sentiment\n",
            "0    5000\n",
            "1    5000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize CountVectorizer\n",
        "vectorizer = CountVectorizer(max_features=5000)  # Use top 5000 features\n",
        "\n",
        "# Fit and transform on training data\n",
        "X_train_bow = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform test data\n",
        "X_test_bow = vectorizer.transform(X_test)\n",
        "\n",
        "print(\"=== BAG-OF-WORDS MODEL CREATED ===\")\n",
        "print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
        "print(f\"Training data shape: {X_train_bow.shape}\")\n",
        "print(f\"Testing data shape: {X_test_bow.shape}\")\n",
        "print(f\"\\nSample feature names (first 20):\")\n",
        "print(list(vectorizer.get_feature_names_out())[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvlwwkEwEwjo",
        "outputId": "19ac24fc-3320-4841-a590-0f63cb71480d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== BAG-OF-WORDS MODEL CREATED ===\n",
            "Vocabulary size: 5000\n",
            "Training data shape: (40000, 5000)\n",
            "Testing data shape: (10000, 5000)\n",
            "\n",
            "Sample feature names (first 20):\n",
            "['aaron', 'abandon', 'abc', 'abduct', 'abil', 'abl', 'abomin', 'abort', 'abound', 'aboutbr', 'abraham', 'abrupt', 'abruptli', 'absenc', 'absent', 'absolut', 'absorb', 'absurd', 'abund', 'abus']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Initialize the Naive Bayes classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "\n",
        "# Train the classifier\n",
        "nb_classifier.fit(X_train_bow, y_train)\n",
        "\n",
        "print(\"=== NAIVE BAYES CLASSIFIER TRAINED ===\")\n",
        "print(f\"Classifier type: {type(nb_classifier).__name__}\")\n",
        "print(f\"Training completed successfully!\")"
      ],
      "metadata": {
        "id": "6zlJ-31C0fHG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d73c1e05-4057-402d-fdc5-e5449f6a82f0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== NAIVE BAYES CLASSIFIER TRAINED ===\n",
            "Classifier type: MultinomialNB\n",
            "Training completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2:**\n",
        "1. Evaluate the performance of the model using the following metrics:\n",
        "\n",
        "    a. Accuracy\n",
        "\n",
        "    b. Precision, Recall, and F1-score\n",
        "\n",
        "    c. Confusion Matrix\n",
        "\n",
        "    d. ROC-AUC Score"
      ],
      "metadata": {
        "id": "jabt1hxXFhUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "TLPy_nhdE0UH"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions and prediction probabilities\n",
        "y_pred = nb_classifier.predict(X_test_bow)\n",
        "y_pred_proba = nb_classifier.predict_proba(X_test_bow)[:, 1]  # Probabilities for positive class\n",
        "\n",
        "print(\"=== PREDICTIONS READY FOR EVALUATION ===\")\n",
        "print(f\"Shape of predictions: {y_pred.shape}\")\n",
        "print(f\"Shape of prediction probabilities: {y_pred_proba.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmStz0-HIO2K",
        "outputId": "48ac1cad-5341-42bf-cdf0-14b5a8e1b67c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== PREDICTIONS READY FOR EVALUATION ===\n",
            "Shape of predictions: (10000,)\n",
            "Shape of prediction probabilities: (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"=== ACCURACY ===\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Accuracy percentage: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1CZkFiqITmJ",
        "outputId": "1eaee444-09ba-458f-a800-c239d9c90d2c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ACCURACY ===\n",
            "Accuracy: 0.8398\n",
            "Accuracy percentage: 83.98%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LQpw6wppIWBi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}